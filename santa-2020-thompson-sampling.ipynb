{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# My Thompson Sampling Agent","metadata":{}},{"cell_type":"code","source":"%%writefile submission.py\n\n# Thompson Sampling (beta distribution) with persistent (choose same bandit if prior step  won)\nimport numpy as np\nfrom random import randrange\nfrom scipy.stats import beta\n\nps_a = None\npost_b = None\nbandit = None\ndecay = None\ntotal_reward = 0\n\nDECAY_RATE = 0.95\n\n\ndef agent(observation, configuration):\n    global reward_sums, total_reward, bandit, post_a, post_b, decay\n    \n    n_bandits = configuration.banditCount\n    r = 0\n    \n    if observation.step == 0:\n        post_a = np.ones(n_bandits)\n        post_b = np.ones(n_bandits)\n        decay = np.ones(n_bandits)\n        bandit = randrange(n_bandits)  # first step choose random\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n        \n        decay[observation.lastActions[0]] *= DECAY_RATE\n        decay[observation.lastActions[1]] *= DECAY_RATE\n\n        post_a[bandit] += r + decay[bandit]\n        post_b[bandit] += (1 - r)\n    \n    if r == 0:  # if prior step lost - choose from beta districution. Else choose the same bandit again\n        bound = beta.mean(post_a, post_b) + beta.std(post_a, post_b) * 3\n        bandit = int(np.argmax(bound))\n    \n    return bandit","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile bay_sub_decay.py\n# My first submission\n\nimport numpy as np\nfrom scipy.stats import beta\n\nps_a = None\npost_b = None\nbandit = None\ndecay = None\ntotal_reward = 0\n\nDECAY_RATE = 0.95\n\n\ndef agent(observation, configuration):\n    global reward_sums, total_reward, bandit, post_a, post_b, decay\n    \n    n_bandits = configuration.banditCount\n\n    if observation.step == 0:\n        post_a = np.ones(n_bandits)\n        post_b = np.ones(n_bandits)\n        decay = np.ones(n_bandits)\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n        \n        decay[observation.lastActions[0]] *= DECAY_RATE\n        decay[observation.lastActions[1]] *= DECAY_RATE\n\n        post_a[bandit] += r + decay[bandit]\n        post_b[bandit] += (1 - r)\n\n    bound = beta.mean(post_a, post_b) + beta.std(post_a, post_b) * 3\n    bandit = int(np.argmax(bound))\n    \n    return bandit","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile bay_sub_decay2.py\n# My second submission\n\n# Thompson Sampling (beta distribution) with persistent (choose same bandit if prior step  won)\nimport numpy as np\nfrom random import randrange\nfrom scipy.stats import beta\n\nps_a = None\npost_b = None\nbandit = None\ndecay = None\ntotal_reward = 0\n\nDECAY_RATE = 0.95\n\n\ndef agent(observation, configuration):\n    global reward_sums, total_reward, bandit, post_a, post_b, decay\n    \n    n_bandits = configuration.banditCount\n    r = 0\n    \n    if observation.step == 0:\n        post_a = np.ones(n_bandits)\n        post_b = np.ones(n_bandits)\n        decay = np.ones(n_bandits)\n        bandit = randrange(n_bandits)  # first step choose random\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n        \n        decay[observation.lastActions[0]] *= DECAY_RATE\n        decay[observation.lastActions[1]] *= DECAY_RATE\n\n        post_a[bandit] += r + decay[bandit]\n        post_b[bandit] += (1 - r)\n    \n    if r == 0:  # if prior step lost - choose from beta districution. Else choose the same bandit again\n        bound = beta.mean(post_a, post_b) + beta.std(post_a, post_b) * 3\n        bandit = int(np.argmax(bound))\n    \n    return bandit","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile thompson_decay1.py\n\n## Thompson Sampling Algorithm with decay\n\nfrom scipy.stats import beta\n\nlast_bandit = -1\ntotal_reward = 0\nDECAY_RATE = 0.99\n\nnumbers_of_rewards_0 = None\nnumbers_of_rewards_1 = None\nchosen_bandits = None\n\ndef thompson_agent_decay1(observation, configuration):    \n    global numbers_of_rewards_1, numbers_of_rewards_0, last_bandit, total_reward, decay\n    \n    d = configuration.banditCount\n\n    if observation.step == 0:\n        numbers_of_rewards_1 = [0] * d\n        numbers_of_rewards_0 = [0] * d\n        decay = [1] * d\n    else:\n        decay[observation.lastActions[0]] *= DECAY_RATE\n        decay[observation.lastActions[1]] *= DECAY_RATE\n\n        reward = observation.reward - total_reward\n        total_reward += reward\n        \n        numbers_of_rewards_1[last_bandit] += reward\n        numbers_of_rewards_0[last_bandit] += (1 - reward)\n        \n\n    bandit = 0\n    max_beta = 0\n    for i in range(0, d):\n        a = 1 + numbers_of_rewards_1[i] * decay[i]\n        b = numbers_of_rewards_0[i] + 1\n        beta_dist = beta(a, b)\n        beta_val = beta_dist.mean() \n        if beta_val > max_beta:\n            max_beta = beta_val\n            bandit = i\n            last_bandit = bandit   \n    \n    #print (f\"last action is {observation.lastActions}. max_radnom is {max_random}. bandit chosen {chosen_bandits[bandit]} times.\")\n    \n    return bandit","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile thompson_decay2.py\n\nfrom scipy.stats import beta\n\nlast_bandit = -1\ntotal_reward = 0\nDECAY_RATE = 0.98\n\nnumbers_of_rewards_0 = None\nnumbers_of_rewards_1 = None\nchosen_bandits = None\n\ndef thompson_agent_decay2(observation, configuration):    \n    global numbers_of_rewards_1, numbers_of_rewards_0, last_bandit, total_reward, decay\n    \n    d = configuration.banditCount\n\n    if observation.step == 0:\n        numbers_of_rewards_1 = [0] * d\n        numbers_of_rewards_0 = [0] * d\n        decay = [1] * d\n    else:\n        decay[observation.lastActions[0]] *= DECAY_RATE\n        decay[observation.lastActions[1]] *= DECAY_RATE\n\n        reward = observation.reward - total_reward\n        total_reward += reward\n        \n        numbers_of_rewards_1[last_bandit] += reward\n        numbers_of_rewards_0[last_bandit] += (1 - reward)\n        \n\n    bandit = 0\n    max_beta = 0\n    for i in range(0, d):\n        a = numbers_of_rewards_1[i] + decay[i]\n        b = numbers_of_rewards_0[i] + 1\n        beta_dist = beta(a, b)\n        beta_val = beta_dist.mean()\n        if beta_val > max_beta:\n            max_beta = beta_val\n            bandit = i\n            last_bandit = bandit   \n    \n    #print (f\"last action is {observation.lastActions}. max_radnom is {max_random}. bandit chosen {chosen_bandits[bandit]} times.\")\n    \n    return bandit","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Other agents for competition","metadata":{}},{"cell_type":"code","source":"%%writefile random_agent.py\n\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile always_first_agent.py\n\ndef always_first(observation, configuration):\n    return 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile ucb_agent.py\n## Upper Confidence Bound\n\nimport math\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\n\ndef ucb_agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward\n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration[\"banditCount\"]\n        sums_of_reward = [0] * configuration[\"banditCount\"]\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n\n    bandit = 0\n    max_upper_bound = 0\n    for i in range(0, configuration.banditCount):\n        if (numbers_of_selections[i] > 0):\n            average_reward = sums_of_reward[i] / numbers_of_selections[i]\n            delta_i = math.sqrt(2 * math.log(observation.step+1) / numbers_of_selections[i])\n            upper_bound = average_reward + delta_i\n        else:\n            upper_bound = 1e400\n        if upper_bound > max_upper_bound and last_bandit != i:\n            max_upper_bound = upper_bound\n            bandit = i\n            last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile bay_sub.py\n\n# Thompson sampling\n\nimport numpy as np\nfrom scipy.stats import beta\n\nps_a = None\npost_b = None\nbandit = None\ntotal_reward = 0\n\n\ndef agent(observation, configuration):\n    global reward_sums, total_reward, bandit, post_a, post_b\n    \n    n_bandits = configuration.banditCount\n\n    if observation.step == 0:\n        post_a = np.ones(n_bandits)\n        post_b = np.ones(n_bandits)\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n\n        post_a[bandit] += r + (1 - observation.step / 2000)\n        post_b[bandit] += (1 - r)\n\n    \n    bound = post_a / (post_a + post_b).astype(float) + beta.std(post_a, post_b) * 3\n    bandit = int(np.argmax(bound))\n    \n    return bandit","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Run competitions","metadata":{}},{"cell_type":"code","source":"! pip install --upgrade pip\n! pip install kaggle-environments --upgrade -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\n\nenv.run([\"bay_sub1.py\", \"agent2.py\"])\nenv.render(mode=\"ipython\", width=800, height=800)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}