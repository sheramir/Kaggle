{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Official Ultralytics YoloV5 Kaggle notebook: [Yolov5-kaggle](https://www.kaggle.com/ultralytics/yolov5-ultralytics)\n\nYolov5 implementation notebook: [Yolo v5 Object Detection Tutorial](https://jooskorstanje.com/yolov5-training-a-custom-object-detection-model.html)\nTutorial: [https://towardsdatascience.com/yolo-v5-object-detection-tutorial-2e607b9013ef](https://towardsdatascience.com/yolo-v5-object-detection-tutorial-2e607b9013ef)\n\nUltralitics YoloV5 wiki: [https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)\n\nAnother tutorial: [https://medium.com/towards-artificial-intelligence/yolo-v5-object-detection-on-a-custom-dataset-61d478bc08f9](https://medium.com/towards-artificial-intelligence/yolo-v5-object-detection-on-a-custom-dataset-61d478bc08f9)\n\nAnd another: [https://lionbridge.ai/articles/create-an-end-to-end-object-detection-pipeline-using-yolov5/](https://lionbridge.ai/articles/create-an-end-to-end-object-detection-pipeline-using-yolov5/)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport pydicom as dicom\nimport cv2\nimport os\nfrom os.path import join, exists","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Get Data\n### I use a revised CSV file with some extra fields I added in previous execution.\n### (The code generated these fields is in comments some blocks ahead.)\n- w,h: width and height of the bounding box (bbox) in pixels.\n- rad_score: Radiologists score according to number of abnormalities each radiologist found. Higher score means the radiologist detected more abnormalities, thus he may be more professional and more accurate (my assumption).\n- img_w, img_h: the original image width and height.\n- w_r, h_r: relative width and height of the bbox, in relative to the image size (betwen 0-1).\n- area_r - relative bbox area size.\n- c_x, c_y - relative x,y center location of the bbox.","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/vinbigdata-chest-xray-abnormalities-detection/'\npath_csv_new = '/kaggle/input/vinbigdataxraytraincsvnew/'\npath_train = join(path,'train')\n\ntrain_df = pd.read_csv(join(path_csv_new,'train_new.csv'))  # Revised train.csv version with extra fields I added\nsamp_subm = pd.read_csv(path+'sample_submission.csv')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Number train samples:', len(train_df.index))\nprint('Number test samples:', len(samp_subm.index))\ntrain_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Get a list of class names in order of class id","metadata":{}},{"cell_type":"code","source":"# Get label classes\nclass_ids, class_names = list(zip(*set(zip(train_df.class_id, train_df.class_name))))\nclasses = list(np.array(class_names)[np.argsort(class_ids)])\nclasses = list(map(lambda x: str(x), classes))\nclasses","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Split the data to 2 Dataframes - DF of images that include labels, and DF with images that wasn't detected any abnormalities.","metadata":{}},{"cell_type":"code","source":"## detected_df was generated and saved in previous version, so I now read it from database.\n\n#detected_df = train_df[train_df.class_id  < 14].sort_values(by=['image_id','class_id','rad_score'], ascending=False)\n#detected_df = detected_df[detected_df.area_r < 0.7] # Remove bboxes that bigger than 70% of the image (not helpfull)\n\ndetected_df = pd.read_csv(join(path_csv_new,'detected_df.csv'), index_col=0) # DF of labeled images with grouping info (the code appears later in comments)\n\nnolabel_df = train_df[train_df.class_id  == 14].copy()\nprint('Number of detections:', len(detected_df.index))\ndetected_df.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Get lists of image names.\n- list_of_images - list of all images\n- list_of_images_no_label - list of images without labels\n- list_of_images_labled - list of images with labels","metadata":{}},{"cell_type":"code","source":"list_of_images = train_df['image_id'].unique().tolist()\nlist_of_images_no_label = nolabel_df['image_id'].unique().tolist()\nlist_of_images_labled = detected_df['image_id'].unique().tolist()\n\nprint ('Total number of images: ', len(list_of_images))\nprint ('Number of images without labels: ', len(list_of_images_no_label))\nprint ('Number of images with labels: ', len(list_of_images_labled))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Find groups of similar bboxes, and create average bboxes\n### Mark bbox type:\n- Single: Only one bbox of that class is in the area of the image.\n- Group#: bbox is part of a group # of similar bboxes (same class, matching IOU), detected by different radiologists.\n- Average#: bbox is created by average of group #.","metadata":{}},{"cell_type":"code","source":"# Calculate IOU (Intersection Over Union) of two bounding boxes\n#   box[0] - x_min\n#   box[1] - y_min\n#   box[2] - x_max\n#   box[3] - y_max\n\ndef bb_iou (boxA, boxB):\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    # compute the area of intersection rectangle\n    interArea = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n    if interArea == 0:\n        return 0\n    # compute the area of both boxes\n    boxAArea = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n    boxBArea = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n    # compute the intersection over union\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    return iou","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Find groups of bboxes by calculating IOU between pairs of same classes.\n# ## This block is commented out after generating the data and creating the new detected_df.csv\n\n# detected_df['bbox_type'] = ''  # Create new field bbox_type for grouping info\n\n# from tqdm.auto import tqdm\n\n# # Set thresholds\n# IOU_threshold = 0.4 # IOU threshold that consider 2 bboxes (of the same class) as being the same annotation\n\n# for image_id in tqdm(list_of_images_labled):\n#     image_data = detected_df[detected_df['image_id'] == image_id]  # Dataframe containing all rows for the specific image   \n    \n#     list_of_classes = image_data['class_id'].unique().tolist() # All class id's in the image\n   \n#     for class_id in list_of_classes:\n#         # Combine ovelapping bboxes with same class_id, and separate them from non overlapping bboxes\n#         class_id_bboxes = image_data[image_data['class_id']==class_id].sort_values('rad_score', ascending=False) # DF of rows with same class_id\n#         group = []\n#         group_indx = []\n#         group_num=-1  # Count different groups of the same class\n        \n#         for i1, row1 in class_id_bboxes.iterrows():\n#             if i1 in class_id_bboxes.index:\n#                 boxA = (row1.x_min, row1.y_min ,row1.x_max, row1.y_max)\n#                 row_dict = row1.to_dict()\n#                 group.append(boxA)\n#                 group_indx.append(i1)\n#                 class_id_bboxes.drop(i1,inplace=True, errors='ignore')\n#                 for i2, row2 in class_id_bboxes.iterrows():  # search for other boxes that overlap with boxA\n#                     boxB = (row2.x_min, row2.y_min ,row2.x_max, row2.y_max)\n#                     if bb_iou (boxA, boxB) > IOU_threshold:\n#                         group.append(boxB)\n#                         group_indx.append(i2)\n#                         class_id_bboxes.drop(i2,inplace=True,errors='ignore')\n#                 if len(group)==1:\n#                     # update bbox_type to 'Single'\n#                     detected_df.loc[group_indx, 'bbox_type'] = 'Single'\n#                 elif len(group)>1:\n#                     #need to create new row with image_id and update bbox_type for the group and average\n#                     group_num += 1\n#                     detected_df.loc[group_indx, 'bbox_type'] = 'Group'+str(group_num)\n#                     row_dict['bbox_type'] = 'Average'+str(group_num)\n#                     row_dict['x_min'] = round(sum([i[0] for i in group])/len(group))\n#                     row_dict['y_min'] = round(sum([i[1] for i in group])/len(group))\n#                     row_dict['x_max'] = round(sum([i[2] for i in group])/len(group))\n#                     row_dict['y_max'] = round(sum([i[3] for i in group])/len(group))\n#                     row_dict['w'] = row_dict['x_max'] - row_dict['x_min']\n#                     row_dict['h'] = row_dict['y_max'] - row_dict['y_min']\n#                     row_dict['w_r'] = round(row_dict['w'] / row_dict['img_w'], 3)\n#                     row_dict['h_r'] = round(row_dict['h'] / row_dict['img_h'], 3)\n#                     row_dict['area_r'] = round(row_dict['w_r'] * row_dict['h_r'],3)\n#                     row_dict['c_x'] = round((row_dict['x_min'] + row_dict['w']/2) / row_dict['img_w'], 3)\n#                     row_dict['c_y'] = round((row_dict['y_min'] + row_dict['h']/2) / row_dict['img_h'], 3)\n#                     #detected_df = detected_df.append(row_dict, ignore_index=True)\n#                     detected_df.loc[detected_df.index.max() + 1] = row_dict\n                    \n#                 group=[]\n#                 group_indx = []\n                  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# detected_df = detected_df.sort_values(by=['image_id','class_id','bbox_type'], ascending=False)\n# detected_df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Explore the Data\n## Exploring distribution of labels","metadata":{}},{"cell_type":"code","source":"# Plot label distribution\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 4))\nx = train_df['class_name'].value_counts().keys()\ny = train_df['class_name'].value_counts().values\nax.bar(x, y)\nax.set_xticklabels(x, rotation=90)\nax.set_title('Distribution of the labels')\nplt.grid()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nlabels = classes[:-1]\norig_vals = []\ntotal_vals = []\nno_groups_vals = []\navg_vals = []\n\nfor label in labels:\n    temp_df = detected_df[detected_df['class_name']==label]\n    total = len(temp_df.index) # total number of labels\n    avgs = len(temp_df[temp_df['bbox_type'].str.contains(\"Average\")].index)  # labels creatged by average of groups\n    singles = len(temp_df[temp_df['bbox_type']=='Single'].index)\n    orig_vals.append(total-avgs)\n    total_vals.append(total)\n    no_groups_vals.append(avgs+singles)\n    avg_vals.append(avgs)\n    \n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.23  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(12, 8))\n# rects: orig(no avg), total(orig+avg), no_group(avg+single), avg(only)\nrects1 = ax.bar(x - width*1.5, orig_vals, width, label='Original')\nrects2 = ax.bar(x - width/2, total_vals, width, label='Original+Avg')\nrects3 = ax.bar(x + width/2, no_groups_vals, width, label='No_Groups')\nrects4 = ax.bar(x + width*1.5, avg_vals, width, label='Averages only')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Sums')\nax.set_title('Distribution of Labels')\nax.set_xticks(x)\nax.set_xticklabels(labels, rotation=90)\nax.legend()\n\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\nautolabel(rects1)\nautolabel(rects2)\nautolabel(rects3)\nautolabel(rects4)\n\nfig.tight_layout()\nplt.grid()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\n\nsns.catplot(data=detected_df, kind=\"violin\", x=\"class_name\", y=\"area_r\",  split=False, height=8.27, aspect=11.7/8.27)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detected_df[detected_df['area_r']>0.4]\n# Conclusion - need to remove detections with area_r > 0.4 - cover too much general area","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detected_df['AR'] = detected_df['w']/ detected_df['h']  # add bbox Aspect-Ratio parameter\ndetected_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.catplot(data=detected_df, kind=\"violin\", x=\"class_name\", y=\"AR\",  split=False, height=8.27, aspect=11.7/8.27)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detected_df[detected_df['AR']>10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove not helpfull labels\ndetected_df = detected_df[detected_df['area_r']<0.4]\ndetected_df = detected_df[detected_df['AR']<10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking the distibution of radiologists\n# The idea is that radiologists that are more active (more labelings) would probably be more accurate\n# so later when I choose a reference bbox to combine other bboxes I would choose the reference\n# from the labling of the higher scored radiologist.\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 4))\nx = detected_df['rad_id'].value_counts().keys()\ny = detected_df['rad_id'].value_counts().values\nax.bar(x, y)\nax.set_xticklabels(x, rotation=90)\nax.set_title('Distribution of the radiologists')\nplt.grid()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Add some columns\n### This code was used to generate train_new.csv and is now in comments.","metadata":{}},{"cell_type":"code","source":"# # add width, height to df\n# train_df['w'] = train_df['x_max'] - train_df['x_min']\n# train_df['h'] = train_df['y_max'] - train_df['y_min']\n# #train_df = train_df[train_df['w'] > 1500 ]\n\n# train_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Checking the distibution of radiologists\n# # The idea is that radiologists that are more active (more labelings) would probably be more accurate\n# # so later when I choose a reference bbox to combine other bboxes I would choose the reference\n# # from the labling of the higher scored radiologist.\n\n# fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n# non_empty_train_df = train_df[train_df[\"class_id\"]!=14]\n# x = non_empty_train_df['rad_id'].value_counts().keys()\n# y = non_empty_train_df['rad_id'].value_counts().values\n# ax.bar(x, y)\n# ax.set_xticklabels(x, rotation=90)\n# ax.set_title('Distribution of the radiologists')\n# plt.grid()\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# rad_list = x.tolist() # Radiologists that labled abnormalities\n# rad_scores = list(range((len(rad_list)),0,-1))  # Score of radiologists according to the number of abnormalities found\n\n# all_rads = train_df['rad_id'].unique().tolist()  # All Radiologists in the dataframe\n# rad_no_score = [i for i in all_rads if i not in rad_list]  # Radiologists that didn't find any abnormality. They get 0 score.\n\n# rad_scores.extend([0]*len(rad_no_score))\n# rad_list.extend(rad_no_score)   # Add radiologists with score 0\n\n# train_df[\"rad_score\"] = train_df[\"rad_id\"].replace(rad_list, rad_scores) # Add new column with that Radiologist score\n# train_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Get images size - img_w, img_h \n# train_df = train_df.assign(img_w=np.nan, img_h=np.nan)\n\n# from tqdm.auto import tqdm\n\n# list_of_images = train_df['image_id'].unique().tolist()\n\n# # Read dicom files and fetch image size(columns,rows) - this is time consuming!\n# for image_id in tqdm(list_of_images):\n#     dicom_path = os.path.join(path,'train',image_id+'.dicom')\n#     data_file = dicom.dcmread(dicom_path)   \n#     train_df.loc[train_df.image_id==image_id, 'img_w'] = data_file.Columns\n#     train_df.loc[train_df.image_id==image_id, 'img_h'] = data_file.Rows\n\n# train_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # add relative window width, relative window height and relative area to df\n# train_df['w_r'] = round(train_df['w'] / train_df['img_w'], 3)\n# train_df['h_r'] = round(train_df['h'] / train_df['img_h'], 3)\n# train_df['area_r'] = round(train_df['h_r'] * train_df['w_r'], 3)\n\n# # add window relative center c_x, c_y\n# train_df['c_x'] = round(((train_df['w'] / 2 ) + train_df['x_min'])/train_df['img_w'], 3)\n# train_df['c_y'] = round(((train_df['h'] / 2 ) + train_df['y_min'])/train_df['img_h'], 3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Write new CSV file \n# from os import makedirs\n# working_path = '/kaggle/working'\n# csv_new_path = join(working_path,'csv_new')\n# makedirs(csv_new_path, exist_ok = True)\n\n# train_new_file = join(csv_new_path,'train_new.csv')\n# detected_df_file = join(csv_new_path,'detected_df.csv')\n# #train_df.to_csv(train_new_file, index=False)\n# detected_df.to_csv(detected_df_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Read DICOM image files","metadata":{}},{"cell_type":"markdown","source":"## Define Dicom class\n- Read dicom file\n- Get image\n- Get bounding boxes\n- Create augmentation\n- Save image","metadata":{}},{"cell_type":"code","source":"import cv2\nimport imgaug as ia\nfrom imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\nfrom imgaug import augmenters as iaa\nfrom matplotlib import colors\nimport random\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nfrom os.path import join\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Dicom_image:\n    def __init__(self, path, image_name, classes=[]):\n        '''\n        Import image\n        :param path: Path to the image\n        :param image_name: image name\n        '''\n        self.path = path\n        if image_name.endswith('.dicom'):\n            self.file_name = image_name\n            self.image_name = image_name[:-6]\n        else:\n            self.file_name = image_name + '.dicom'\n            self.image_name = image_name\n        self.file_path = join(self.path, self.file_name)\n        if exists(self.file_path):\n            self.dicom_file = dicom.dcmread(self.file_path)\n            self.image_orig = self.read_xray()\n            self.orig_size = (self.image_orig.shape[0],self.image_orig.shape[1]) # Original image size before resizing - (h,w)\n            self.bbs = [] # Initial bounding boxes\n            self.classes = classes\n            self.image_aug = None # Augmented image\n            self.bbs_aug = []  # bounding boxes after augmentation\n            self.image_resize = None # image after resize\n            self.bbs_resize= [] # bounding boxes after resize\n        else:\n            print (f\"Error! {self.file_path} does not exists!\")\n    \n    def read_xray(self, voi_lut = True, fix_monochrome = True):\n        '''\n        Read image data from the dicom file\n        (reference: https://www.kaggle.com/raddar/vinbigdata-competition-jpg-data-2x-downsampled)\n        '''\n        data_file = self.dicom_file\n        # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n        if voi_lut:\n            data = apply_voi_lut(data_file.pixel_array, data_file)\n        else:\n            data = data_file.pixel_array\n\n        # depending on this value, X-ray may look inverted - fix that:\n        if fix_monochrome and data_file.PhotometricInterpretation == \"MONOCHROME1\":\n            data = np.amax(data) - data\n        data = data - np.min(data)\n        data = data / np.max(data)\n        data = (data * 255.0).astype(np.uint8)\n        # Use CLAHE (Contrast Limited Adaptive Histogram Equalization) to improve contrast\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n        data = clahe.apply(data)\n        data = cv2.cvtColor(data,cv2.COLOR_GRAY2RGB) # convert to 3 channel grayscale\n        return data\n\n    def add_bbox(self, xmin, ymin, xmax, ymax, cl_label):\n        '''\n        Add a bounding box to list of bounding boxes in the image\n        :param xmin, ymin, xmax, ymax: bounding box coordinates\n        :param cl_label: string label class of the bounding box\n        '''\n        self.bbs.append(BoundingBox(x1=xmin, y1=ymin, x2=xmax, y2=ymax, label=cl_label))\n    \n    def remove_bbox(self, bbox):\n        '''\n        Remove a bounding box from the list of bounding boxes\n        :param bbox: a list of bbox coordinates and label in the form [xmin, ymin, xmax, ymax, cl_label]\n        '''\n        if bbox in self.bbs:\n            self.bbs.remove(bbox)\n            success = True\n        else:\n            success = False\n        return success\n    \n    def minmax_bbox(self, bbs=None):\n        '''\n        Find min and max values of bounding boxes (so cropping will not cut the bboxes)\n        :param bboxs (default: self.bbs): list of bounding boxes\n        '''\n        bbs = self.bbs if bbs is None else bbs\n        bbs_xmax = max([bbs[i].x2_int for i in range(len(bbs))])\n        bbs_ymax = max([bbs[i].y2_int for i in range(len(bbs))])\n        bbs_xmin = min([bbs[i].x1_int for i in range(len(bbs))])\n        bbs_ymin = min([bbs[i].y1_int for i in range(len(bbs))])        \n        return bbs_xmin, bbs_ymin, bbs_xmax, bbs_ymax\n    \n    def sort_bbs(self):\n        '''\n        Sort the bbs list of bounding boxes by the class label\n        :param none\n        '''\n        def sort_class(elem):\n            return elem.label\n        self.bbs.sort(key = sort_class)\n        return self.bbs\n    \n    def resize (self, img=None, bounding_boxes=None, downscale_factor = 1, max_dim = None):\n        '''\n        Resize image\n        :param downscale_factor: downscale factor (default = 1)\n        :param max_dim: downscale so that largest dimension (height or width) is equal max_dim. The other dimension reduced proportionately.\n        '''\n        img = self.image_orig if img is None else img\n        img_size = (img.shape[0], img.shape[1])\n        bounding_boxes = self.bbs if bounding_boxes is None else bounding_boxes\n        if type(bounding_boxes)==BoundingBoxesOnImage:\n            bbs = bounding_boxes\n        else:\n            bbs = BoundingBoxesOnImage(bounding_boxes, shape=img.shape)\n        \n        if downscale_factor > 1:  # Resize by a constant factor\n            new_shape = tuple([int(x / downscale_factor) for x in img_size])       \n        elif max_dim != None: # Resize so that maximum length (width or height) will be max_dim. The other dimension resized proportionately\n            downscale_factor =  max(img.shape) / max_dim\n            new_shape = tuple([round(x / downscale_factor) for x in img_size])\n        else:\n            new_shape = img_size # Don't resize\n\n        self.image_resize = ia.imresize_single_image(img, new_shape)\n        self.bbs_resize = bbs.on(self.image_resize)\n        return self.image_resize, self.bbs_resize\n    \n    def augment(self, img=None, bounding_boxes=None, keep_size=False):\n        '''\n        Augment the image\n        :param img: Image to be processed\n        :param bounding_boxes: Bounding boxes (default is the original image bboxes).\n        :param keep_size: Keep the size of the image after cropping (default is False - don't resize to original size)\n        '''\n        img = self.image_orig if img is None else img\n        bounding_boxes = self.bbs if bounding_boxes is None else bounding_boxes\n        if type(bounding_boxes)==BoundingBoxesOnImage:\n            bbs = bounding_boxes\n        else:\n            bbs = BoundingBoxesOnImage(bounding_boxes, shape=img.shape)\n        \n        # Do basic augmentation - brightness, scale and shear\n        seq = iaa.Sequential([\n            iaa.MultiplyBrightness((0.7, 1.2)),\n            iaa.Affine(scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}),\n            iaa.Affine(shear=(-10, 10))\n        ])\n        image_aug, bbs_aug = seq(image=img, bounding_boxes=bbs)\n        \n        # Do random Cropping up to 20% of all sides\n        # First - find safe margins for cropping so that bboxes won't be cropped out\n        if not bbs_aug.empty:\n            max_percent = 0.2\n            w = image_aug.shape[1]\n            h = image_aug.shape[0]\n            bbs_xmin, bbs_ymin, bbs_xmax, bbs_ymax = self.minmax_bbox(bbs=bbs_aug)\n            marg_left = (bbs_xmin -2) / w\n            marg_right = (w - bbs_xmax -2) / w\n            marg_top = (bbs_ymin -2) / h\n            marg_bottom = (h - bbs_ymax -2) / h\n            max_crop_percent = min(0.3, marg_left, marg_right, marg_top, marg_bottom)\n            max_crop_percent = 0 if max_crop_percent<0 else max_crop_percent\n        crop = iaa.Crop(percent=(0, max_crop_percent), keep_size=keep_size) # crop image\n        image_aug=crop.augment_image(image_aug)\n        bbs_aug = bbs_aug.on(image_aug)\n        \n        self.image_aug, self.bbs_aug = image_aug, bbs_aug\n        return image_aug, bbs_aug\n\n    def parse_label(self, bb):\n        '''\n        Convert label marked as 'c_r' (c=class number, r=radiologist rank) to label title and color\n        :param bb: bounding box with label\n        '''\n        color_pallete = ['lightcoral','brown','red','darksalmon','chocolate',\n                  'darkorange','darkgoldenrod','gold','olive','yellow',\n                  'green','lime','blue','darkorchid']\n        try:\n            class_code = int(bb.label.split('_')[0])\n            class_name = self.classes[class_code]\n        except:\n            class_code = random.randrange(len(colors))\n            class_name = ''\n        cl_color = tuple(np.array(colors.to_rgb(color_pallete[class_code]))*255)\n        return cl_color, class_name\n    \n    def show_image(self, img=None, bounding_boxes=None, showbbs=True, showlabel=True):\n        '''\n        Show the image with bounding boxes\n        :param img: image to be processed\n        :param bounding_boxes: list of bounding boxes\n        :param showbbs: flag to show the bounding boxes, if there are any (default=True)\n        :param showlabel: flag to show the labels of the bounding boxes, if there are any (default=True)\n        '''\n        img = self.image_orig if img is None else img\n        bounding_boxes = self.bbs if bounding_boxes is None else bounding_boxes\n        bbs=[]\n        if bounding_boxes != []:\n            if type(bounding_boxes)==BoundingBoxesOnImage:\n                bbs = bounding_boxes.bounding_boxes\n            else:\n                bbs = bounding_boxes\n            \n            for bb in bbs:\n                bbox_color, bbox_label = self.parse_label(bb)\n                if showlabel:\n                    bbl = bb.copy(label=bbox_label)\n                    img = bbl.draw_on_image(image=img, color=bbox_color)\n                elif showbbs:\n                    img = bb.draw_box_on_image(image=img, color=bbox_color)     \n        ia.imshow(img)\n        \n    def save_image_label (self, fname=None, img_path='', label_path='', img=None):\n        '''\n        Save the image as jpg and bounding boxes as txt\n        :param fname: file name (without extension - jpg and txt will be added to fname automatically)\n        :param img: what image to save - 'orig' or None = original image; 'aug' = augmented image; 'resize' = resized image\n        :param bounding_boxes: list of bounding boxes\n        :param savelabel: flag to save label text file (default=True)\n        '''\n        fname = self.image_name if fname is None else fname\n        \n        if ('aug' in img):\n            image = self.image_aug\n            bounding_boxes = self.bbs_aug\n        elif ('resize' in img):\n            image = self.image_resize\n            bounding_boxes = self.bbs_resize\n        else:\n            image = self.image_orig\n            bounding_boxes = self.bbs\n        if image is None:  # img='aug' or 'resize' but augmentation/resize was not performed before.\n            image = self.image_orig\n            bounding_boxes = self.bbs\n            print (f'Warning: Saving original image. {img} was not performed on image!')\n            \n        self.save_image(fname = fname, img_path=img_path, img=image)\n        self.save_label(img_size=image.shape, fname = fname, label_path=label_path, bounding_boxes=bounding_boxes)\n        \n                \n    def save_image(self, fname = None, img_path=None, img=None):\n        '''\n        Save the image as jpg\n        :param fname: file name (without extension - jpg will be added to fname automatically)\n        :param img: image to be saved\n        '''\n        fname = self.image_name if fname is None else fname\n        img = self.image_orig if img is None else img\n        image_path = join(img_path, fname+'.jpg')\n        status=cv2.imwrite(image_path, img)\n        if not status:\n            print (f'Error: Could not save {fname}.jpg to {img_path}!')\n            \n        \n    def save_label(self, img_size, fname = None, label_path='', bounding_boxes=None):\n        '''\n        Save the bounding boxes as txt\n        :param fname: file name (without extension - jpg and txt will be added to fname automatically)\n        :param bounding_boxes: list of bounding boxes\n        :param savelabel: flag to save label text file (default=True)\n        '''\n        fname = self.image_name if fname is None else fname\n        bounding_boxes = self.bbs if bounding_boxes is None else bounding_boxes\n        bbs=[]\n        if bounding_boxes != []:\n            if type(bounding_boxes)==BoundingBoxesOnImage:\n                bbs = bounding_boxes.bounding_boxes\n            else:\n                bbs = bounding_boxes\n        \n        lbl_path = join(label_path, fname+'.txt')\n        \n        with open(lbl_path, 'w+') as file:\n            for bb in bbs:\n                bb = bb.clip_out_of_image(img_size)\n                xc = round(bb.center_x / img_size[1], 5)\n                yc = round(bb.center_y / img_size[0], 5)\n                w = round(bb.width / img_size[1], 5)\n                h = round(bb.height / img_size[0], 5)\n                class_id = bb.label.split('_')[0]\n                line = ' '.join((str(class_id), str(xc), str(yc), str(w), str(h))) + '\\n'\n                file.write(line) \n        \n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_dicom(fpath, image_id, df):\n    dic = Dicom_image(path=fpath, image_name=image_id, classes=classes)\n    image_df = df[df['image_id']==image_id]\n    for i, row in image_df.iterrows():\n        dic.add_bbox(row.x_min, row.y_min ,row.x_max, row.y_max,str(row.class_id))\n    dic.resize(max_dim=1024)\n    dic.show_image(img=dic.image_resize, bounding_boxes=dic.bbs_resize, showlabel=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_dicom(fpath=path_train, image_id='051132a778e61a86eb147c7c6f564dfe',df=detected_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%script echo skipping\n# test timing of augment:\n#1. Augment full size, Resize to 512\n#2. Resize half size. Augment. Resize to 512.\n#3. Resize to 512. Augment with keep_size=True (lower quality)\n\nimport timeit\n\ndic = Dicom_image(path=path+'train', image_name='051132a778e61a86eb147c7c6f564dfe', classes=classes)\ndic.add_bbox(500,700,1000,2000,'9')\ndic.add_bbox(900,500,1200,1900,'2')\ndic.add_bbox(100,800,1100,1200,'4')\ndic.add_bbox(1500,2000,2200,2700,'5')\n \n# Test 1 - Augment full size, Resize to 512\nstarttime = timeit.default_timer()\ndic.augment()\ndic.resize(img=dic.image_aug, bounding_boxes=dic.bbs_aug, max_dim=512)\ndic.show_image(img=dic.image_resize, bounding_boxes=dic.bbs_resize)\nprint(\"Test1 time difference is :\", timeit.default_timer() - starttime)\n\n# Test 2 - Resize half size. Augment. Resize to 512.\nstarttime = timeit.default_timer()\ndic.resize(downscale_factor = 2)\ndic.augment(img=dic.image_resize, bounding_boxes=dic.bbs_resize)\ndic.resize(img=dic.image_aug, bounding_boxes=dic.bbs_aug, max_dim=512)\ndic.show_image(img=dic.image_resize, bounding_boxes=dic.bbs_resize)\n\nprint(\"Test2 time difference is :\", timeit.default_timer() - starttime)\n\nfrom os import makedirs\nworking_path = '/kaggle/working'\ntestpath = join(working_path,'test')\nmakedirs(testpath, exist_ok = True)\n\ndic.save_image_label (fname='test_aug', img_path=testpath, label_path=testpath, img='aug')\ndic.save_image_label (fname='test_resize', img_path=testpath, label_path=testpath, img='resize')\n\n# Test 3 - Resize to 512. Augment with keep_size=True\nstarttime = timeit.default_timer()\ndic.resize(max_dim=512)\ndic.augment(img=dic.image_resize, bounding_boxes=dic.bbs_resize, keep_size=True)\ndic.show_image(img=dic.image_aug, bounding_boxes=dic.bbs_aug)\nprint(\"Test2 time difference is :\", timeit.default_timer() - starttime)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Show examples of each class","metadata":{}},{"cell_type":"code","source":"%%script echo skipping\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef plot_example(df, idx_list):\n    fig, axs = plt.subplots(1, 3, figsize=(15, 10))\n    fig.subplots_adjust(hspace = .1, wspace=.1)\n    axs = axs.ravel()\n    for i in range(3):\n        image_id = df.loc[idx_list[i], 'image_id']\n        img,_ = read_xray(os.path.join(path,'train',image_id+'.dicom'))\n        axs[i].imshow(img, cmap='gray')\n        axs[i].set_title(df.loc[idx_list[i], 'class_name'])\n        axs[i].set_xticklabels([])\n        axs[i].set_yticklabels([])\n        if df.loc[idx_list[i], 'class_name'] != 'No finding':\n            bbox, anchor, w, h = get_bbox(df, idx_list[i])\n            p = matplotlib.patches.Rectangle(anchor, w,h,ec='r', fc='none', lw=2.)\n            axs[i].add_patch(p)\n\n# Show examples: Remove the comments to show the examples            \n#for num in range(15):\n    #idx_list = train_df[train_df['class_id']==num][0:3].index.values\n    #plot_example(train_df, idx_list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setting the environment for YOLOV5\n## Set up the directories\nYolo V5 needs a very specific set up of data folders in order to work:\n\n![](https://miro.medium.com/max/472/1*XupA8TGTSGdZdjsrs16hkw.png)","metadata":{}},{"cell_type":"markdown","source":"## Set up the data\n### The images\nThe images have to be directly in the image folders. Training images in the data/images/train folder and validation images in the data/images/valid folder.\nThe names of the images have to be simply unique names with a .jpg (or another format).\n### The labels\nThe labels have to be in the data/labels/train/ or in the data/labels/valid.\nThe name of the labels file has to be the same name as the image, but with “.txt” instead of “.jpg”.\nThe bounding boxes have to be listed as one bounding box per line, with on that line:\n* the class number of the object in the bounding box (always 0 if only one class)\n* the standardized center pixel of the bounding box in terms of width\n* the standardized center pixel of the bounding box in terms of height\n* the standardized width of the bounding box\n* the standardized height of the bounding box","metadata":{}},{"cell_type":"code","source":"%%script echo skipping\n# Calculate IOU (Intersection Over Union) of two bounding boxes\n#   box[0] - x_min\n#   box[1] - y_min\n#   box[2] - x_max\n#   box[3] - y_max\n\ndef bb_iou (boxA, boxB):\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    # compute the area of intersection rectangle\n    interArea = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n    if interArea == 0:\n        return 0\n    # compute the area of both boxes\n    boxAArea = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n    boxBArea = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n    # compute the intersection over union\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    return iou\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint ('Total number of images: ', len(list_of_images))\nprint ('Number of images without labels: ', len(list_of_images_no_label))\nprint ('Number of images with labels before augmentation: ', len(list_of_images_labled))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare dataframe to calculate number of augmentation for each image\n### The purpose is to create balanced dataset ","metadata":{}},{"cell_type":"code","source":"\nno_group_df = detected_df[~detected_df['bbox_type'].str.contains(\"Group\")]\n\nlabeled_df = no_group_df[['image_id','class_name']].copy()\nfor label in classes[:-1]:\n    labeled_df[label] = labeled_df['class_name'].apply(lambda x: int(label in x))\n\n#newdf = labeled_df[labeled_df['image_id']=='ffceb71a80efba3b83c88e11f4b9694b']\nimages_df = labeled_df.groupby(['image_id']).sum()\nimages_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_dicom(fpath=path_train, image_id='051132a778e61a86eb147c7c6f564dfe',df=no_group_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for label in classes[:-1]:\n    images_df[label+'_'] = 0\nimages_df.insert(0,'numaug',1)\nimages_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for label in classes[:-1]:\n    images_df[label+'_'] = images_df['numaug'] * images_df[label]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Checking the distibution of lables. Looking for smallest labels to duplicate (using augmentation)","metadata":{}},{"cell_type":"code","source":"images_df.sum(axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"no_large_classes = (images_df['Pleural thickening']==0) & (images_df['Pulmonary fibrosis']==0) & (images_df['Aortic enlargement']==0) & (images_df['Cardiomegaly']==0) \nimages_df[(images_df['Pneumothorax']>0) & (images_df['Atelectasis']>0) & no_large_classes][classes[:-1]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Trying to reach target of ~3000 detections per label. Need to add ~2800 to 'Atelectasis' and 'Pneumothorax'\n### I don't want to inflate 'Other lesion' so I would augment image af201da8a5f8354c4c3291995d5cbafd 300 times and the rest will be augmented (2800-300)/3 = 834 times","metadata":{}},{"cell_type":"code","source":"images_df.loc[(images_df['Pneumothorax']>0) & (images_df['Atelectasis']>0) & no_large_classes, 'numaug'] = 901 # was 835\nimages_df.loc['af201da8a5f8354c4c3291995d5cbafd', 'numaug'] = 101 # was 301\n\nimages_df[(images_df['Pneumothorax']>0) & (images_df['Atelectasis']>0) & no_large_classes]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Update number of generated labels\nfor label in classes[:-1]:\n    images_df[label+'_'] = images_df['numaug'] * images_df[label]\n    \nimages_df.sum(axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Now looking to duplicate 'Calcification', 'ILD', 'Infiltration'. Need to add ~2500 to them. ","metadata":{}},{"cell_type":"code","source":"check_cols = ['Calcification', 'ILD', 'Infiltration']\nremove_cols = [i for i in labels if i not in check_cols]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_df[(images_df['ILD']>0) & (images_df['Calcification']>0) & no_large_classes][classes[:-1]]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_df.loc['35e38672875ff60d5a131d91b4db5a6d', 'numaug'] += 400\nimages_df.loc['9c314e403d3e6e3ed09e79a57019f9ad', 'numaug'] += 600","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Update number of generated labels\nfor label in classes[:-1]:\n    images_df[label+'_'] = images_df['numaug'] * images_df[label]\n    \nimages_df.sum(axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"no_large_classes2 = (images_df['Pleural thickening']==0) & (images_df['Pulmonary fibrosis']==0) & (images_df['Aortic enlargement']==0) & (images_df['Cardiomegaly']==0) & (images_df['Lung Opacity']==0)\n\nimages_df[(images_df['Consolidation']>0) & (images_df['Infiltration']>0) & no_large_classes2][classes[:-1]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_df[(images_df['Consolidation']==0) & (images_df['Infiltration']>0) & no_large_classes2][classes[:-1]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_df.loc['82877be2465c084b0b9bc186fc7f158f', 'numaug'] += 300\nimages_df.loc['cfbac484f94686cd93d564487d9e5a8a', 'numaug'] += 1000","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Update number of generated labels\nfor label in classes[:-1]:\n    images_df[label+'_'] = images_df['numaug'] * images_df[label]\n    \nimages_df.sum(axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_df[images_df['numaug']>1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_dicom(fpath=path_train, image_id='cfbac484f94686cd93d564487d9e5a8a',df=no_group_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_dicom(fpath=path_train, image_id='af201da8a5f8354c4c3291995d5cbafd',df=no_group_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove unhelpfull 'Other lesion'(9)\nimages_df.loc['af201da8a5f8354c4c3291995d5cbafd','Other lesion']=0\nno_group_df.drop(no_group_df[(no_group_df['image_id']=='af201da8a5f8354c4c3291995d5cbafd') & (no_group_df['class_id']==9)].index, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_dicom(fpath=path_train, image_id='9c314e403d3e6e3ed09e79a57019f9ad',df=no_group_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove 2 larger 'Calcification'(2)\n#print(no_group_df[no_group_df['image_id']=='9c314e403d3e6e3ed09e79a57019f9ad'])\nno_group_df.drop(19138, inplace=True)\nno_group_df.drop(16321, inplace=True)\nimages_df.loc['9c314e403d3e6e3ed09e79a57019f9ad','Calcification']=1\nno_group_df[no_group_df['image_id']=='9c314e403d3e6e3ed09e79a57019f9ad']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_dicom(fpath=path_train, image_id='82877be2465c084b0b9bc186fc7f158f',df=no_group_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove 'Other lesion'(9) and medium 'Infiltration'(6) - index 72294\nprint(no_group_df[no_group_df['image_id']=='82877be2465c084b0b9bc186fc7f158f'])\nno_group_df.drop(72294, inplace=True)\nno_group_df.drop(no_group_df[(no_group_df['image_id']=='82877be2465c084b0b9bc186fc7f158f') & (no_group_df['class_id']==9)].index, inplace=True)\nimages_df.loc['82877be2465c084b0b9bc186fc7f158f','Other lesion']=0\nimages_df.loc['82877be2465c084b0b9bc186fc7f158f','Infiltration']=3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_dicom(fpath=path_train, image_id='76339ec8c17dbcdd117914581cee59f5',df=no_group_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_dicom(fpath=path_train, image_id='6ce61a39f1e1bff629566de047ab8775',df=no_group_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"no_group_df[no_group_df['image_id']=='6ce61a39f1e1bff629566de047ab8775']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_dicom(fpath=path_train, image_id='35e38672875ff60d5a131d91b4db5a6d',df=no_group_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_dicom(fpath=path_train, image_id='106a3da41d2e3d9f508c09b28e8abdaf',df=train_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove 'Other lesion'(9)\nprint(no_group_df[no_group_df['image_id']=='106a3da41d2e3d9f508c09b28e8abdaf'])\nno_group_df.drop(33745, inplace=True)\nimages_df.loc['106a3da41d2e3d9f508c09b28e8abdaf','Other lesion']=0\nno_group_df[no_group_df['image_id']=='106a3da41d2e3d9f508c09b28e8abdaf']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show final classes distribution:\n\n# Update number of generated labels\nfor label in classes[:-1]:\n    images_df[label+'_'] = images_df['numaug'] * images_df[label]\n    \nimages_df.sum(axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save images from dicom\n## Save images for un-labeled dicoms","metadata":{}},{"cell_type":"code","source":"from os import makedirs, listdir\n\n# makedirs('data/images/train', exist_ok = True)\n# makedirs('data/images/valid', exist_ok = True)\n# makedirs('data/labels/train', exist_ok = True)\n# makedirs('data/labels/valid', exist_ok = True)\n\nworking_path = '/kaggle/working'\nimages_path = join(working_path,'images')\nlabels_path = join(working_path,'labels')\nmakedirs(images_path, exist_ok = True)\nmakedirs(labels_path, exist_ok = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nfor image_id in tqdm(list_of_images_no_label):\n    dic = Dicom_image(path=path_train, image_name=image_id, classes=classes)\n    dic.resize(max_dim=512)\n    dic.save_image_label (img_path=images_path, label_path=labels_path, img='resize')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save images for labeled dicoms, including augmentations","metadata":{}},{"cell_type":"code","source":"\nfor image_id in tqdm(list_of_images_labled):\n    image_data_df = no_group_df[no_group_df['image_id'] == image_id]  # Dataframe containing all rows for the specific image\n    #dicom_path = join(path_train, image_id+'.dicom')\n    #img, (img_h, img_w) = read_xray(dicom_path, max_dim = 512) # Read image, resize to 512pt and get original image size h,w for norm\n    \n    dic = Dicom_image(path=path_train, image_name=image_id, classes=classes)  # Read dicom file\n\n    for i, row in image_data_df.iterrows():\n        dic.add_bbox(row.x_min, row.y_min ,row.x_max, row.y_max,str(row.class_id))  # Get bounding boxes\n    \n    dic.resize(max_dim=512)\n    dic.save_image_label (img_path=images_path, label_path=labels_path, img='resize') # Save original image\n    \n    # Create augmentations\n    numaug = images_df.loc[image_id, 'numaug'] - 1\n    if numaug > 0:\n        img_temp, bbs_temp = dic.resize(downscale_factor = 2)\n        for aug in tqdm(range(numaug)):\n            dic.augment(img=img_temp, bounding_boxes=bbs_temp)\n            dic.resize(img=dic.image_aug, bounding_boxes=dic.bbs_aug, max_dim=512)\n            dic.save_image_label (fname=image_id+'_aug'+str(aug), img_path=images_path, label_path=labels_path, img='resize') # Save augmented image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Count files\nprint(\"No. of images\", len(listdir(images_path)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%script echo skipping\ntxtfile = r'data/labels/train/051132a778e61a86eb147c7c6f564dfe.txt'\ndef print_txt_file(txtfile):\n    f = open(txtfile, 'r')\n    file_contents = f.read()\n    print(file_contents)\n    f.close()\n    \nprint_txt_file(txtfile)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}